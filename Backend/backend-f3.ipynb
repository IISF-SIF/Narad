{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7385368,"sourceType":"datasetVersion","datasetId":4111841},{"sourceId":7406242,"sourceType":"datasetVersion","datasetId":4294512},{"sourceId":7425443,"sourceType":"datasetVersion","datasetId":4316316},{"sourceId":158055712,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dowloading all the dependencies","metadata":{}},{"cell_type":"code","source":"from ipywidgets import IntProgress\nfrom IPython.display import display\nimport time\n\nmax_count = 10\n\nf = IntProgress(min=0, max=max_count) # instantiate the bar\ndisplay(f) # display the bar\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain neural-compressor\nf.value += 1\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain flash-attn --no-build-isolation\nf.value += 1\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain bitsandbytes>=0.39.0 accelerate>=0.20.0\nf.value += 1\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain langchain\nf.value += 1\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain pypdf\nf.value += 1\n!python -m pip install -q --no-index --find-links=/kaggle/input/pip-download-for-langchain sentence-transformers\nf.value += 1\n!pip install -q faiss-gpu\nf.value += 1\n!pip install -q indic-nlp-library\nf.value += 1\n!pip install -q -U sacremoses\nf.value += 1\n!pip install -q -U google-generativeai\nf.value += 1\n!pip install -q urduhack\nf.value +=1","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-18T16:42:28.837408Z","iopub.execute_input":"2024-01-18T16:42:28.837888Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, max=10)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a28510db1a6b40a7a3a11819160cc234"}},"metadata":{}},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import git\n\n# Clone the repository from the specified URL to the target directory\nrepo_url = 'https://github.com/VarunGumma/IndicTransTokenizer'\ntarget_directory = '/kaggle/working/sample'\n\n# Clone the repository\ngit.Repo.clone_from(repo_url, target_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n# Append the specified directory to the Python sys.path\ntarget_directory = '/kaggle/working/sample'\nsys.path.append(target_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import required modules and classes\nfrom transformers import BitsAndBytesConfig  # For quantization of the LLM model\nimport torch\nfrom IndicTransTokenizer import IndicProcessor, IndicTransTokenizer # For initialization of the translational model\nfrom transformers import AutoModelForSeq2SeqLM # For initialization of the translational model\nimport transformers  # For the initialization of the LLM Model\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n\n# Import modules for vector database\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate  # Helps in creating a prompt template\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders import TextLoader\n\n# Additional imports\nimport torch\nimport torchaudio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n# Create a quantization configuration using BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization during model loading\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initializing Automatic speech recognition using IndicWav2Vec by AI4Bharat","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"ai4bharat/indicwav2vec-hindi\", device=\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialiazation of the AI4Bharat IndicTrans2\n### This model has amazing translational capabilities which will be helpful to us to answer a wide diversity of people","metadata":{}},{"cell_type":"markdown","source":"### Indic to English","metadata":{}},{"cell_type":"code","source":"# Instantiate an IndicTransTokenizer for transliterating from Indic to English\ntokenizer_indic_to_en = IndicTransTokenizer(direction=\"indic-en\")\n\n# Instantiate an IndicProcessor for processing transliteration from Indic to English\nip_indic_to_en = IndicProcessor(inference=True)\n\n# Load a pre-trained model for transliteration from Indic to English using AutoModelForSeq2SeqLM\nmodel_indic_to_en = AutoModelForSeq2SeqLM.from_pretrained(\n    \"ai4bharat/indictrans2-indic-en-1B\",\n    trust_remote_code=True,\n    device_map=\"cuda:0\",\n    quantization_config=quantization_config  # Quantization configuration for the model\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### English to Indic","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer using IndicTransTokenizer for transliterating from English to Indic\ntokenizer_en_to_indic = IndicTransTokenizer(direction=\"en-indic\")\n\n# Instantiate an IndicProcessor for processing transliteration from English to Indic\nip_en_to_indic = IndicProcessor(inference=True)\n\n# Load a pre-trained model for transliteration from English to Indic using AutoModelForSeq2SeqLM\nmodel_en_to_indic = AutoModelForSeq2SeqLM.from_pretrained(\n    \"ai4bharat/indictrans2-en-indic-dist-200M\",\n    trust_remote_code=True,\n    device_map=\"cuda:0\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization of the LLM Model - Intel Neural Chat v-3.3","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Define the model name\nmodel_name = 'Intel/neural-chat-7b-v3-3-Slerp'\n\n# Load a pre-trained model for causal language modeling using AutoModelForCausalLM\nneural_intel_chat_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"cuda:0\",\n    quantization_config=quantization_config  # Quantization configuration for the model\n)\n\n# Load the corresponding tokenizer using AutoTokenizer\nneural_intel_chat_tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = transformers.pipeline(\"text-generation\", model=neural_intel_chat_model, tokenizer=neural_intel_chat_tokenizer, max_new_tokens=256)\nhf = HuggingFacePipeline(pipeline=pipe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization of Gemini API","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport textwrap\n\n# Import the generative AI module from the google package\nimport google.generativeai as genai\n\n# Import necessary display modules from IPython\nfrom IPython.display import display, Markdown\n\ndef to_markdown(text):\n    \"\"\"\n    Convert plain text to Markdown format.\n\n    This function takes a plain text input and converts it to Markdown format.\n    It also replaces bullet points with proper Markdown list syntax.\n\n    Args:\n        text (str): The plain text to be converted to Markdown.\n\n    Returns:\n        Markdown: The converted Markdown text.\n\n    Example:\n        >>> plain_text = \"This is a bullet point:\\n• Item 1\\n• Item 2\"\n        >>> markdown_output = to_markdown(plain_text)\n        >>> print(markdown_output)\n        > This is a bullet point:\n        >   * Item 1\n        >   * Item 2\n    \"\"\"\n    # Replace special character '•' with proper Markdown list syntax\n    text = text.replace('•', '  *')\n    \n    # Indent the text with '>' to format it as a blockquote\n    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n# Create a UserSecretsClient instance to access Kaggle user secrets\nuser_secrets = UserSecretsClient()\n\n# Retrieve the Google API key from Kaggle user secrets\n# Note: Ensure that you have previously stored the \"Google API key\" as a secret on Kaggle\nsecret_value_0 = user_secrets.get_secret(\"sif\")\n\n# Configure the generative AI module with the obtained Google API key\ngenai.configure(api_key=secret_value_0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a generative language model instance using the 'gemini-pro' model\nllm_model = genai.GenerativeModel('gemini-pro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization of TTS Hindi","metadata":{}},{"cell_type":"code","source":"from transformers import VitsModel, AutoTokenizer\nimport torch\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntts_hindi_model = VitsModel.from_pretrained(\"facebook/mms-tts-hin\").to(device)\ntts_hindi_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-hin\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization of the Vector Database","metadata":{}},{"cell_type":"code","source":"# Define the model name for Hugging Face embeddings\nmodel_name = 'WhereIsAI/UAE-Large-V1'\n\n# Define model kwargs for Hugging Face embeddings during initialization\nmodel_kwargs = {'device': 'cuda:0'}\n\n# Define encode kwargs for Hugging Face embeddings\n# Set 'normalize_embeddings' to True to compute cosine similarity\nencode_kwargs = {'normalize_embeddings': True}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TextLoader instance for loading text documents\n# Specify the path to the text file (\"/kaggle/input/evspdf/RAG.txt\" in this case)\nloader = TextLoader(\"/kaggle/input/rag-dataset-bhuvan/RAGlatestDatabaseNew.txt\")\n\n# Load documents using the TextLoader\ndocuments = loader.load()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a RecursiveCharacterTextSplitter instance for splitting text documents\n# Set a small chunk size and overlap for demonstration purposes\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n    is_separator_regex=False\n)\n\n# Split the loaded documents using the TextSplitter\ntext_docs = text_splitter.split_documents(documents)\n\n# Create Hugging Face embeddings instance\n# Specify the model name, model kwargs, and encode kwargs\nembeddings = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n\n# Create a FAISS vector store from the split text documents and embeddings\ndb = FAISS.from_documents(text_docs, embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conversation History Tools","metadata":{}},{"cell_type":"code","source":"def get_history(k=3):\n    \"\"\"\n    Retrieve conversation history for contextual understanding.\n\n    Parameters:\n    - k (int): The number of conversation tiles to include in the context (default is 3).\n\n    Returns:\n    str: A string containing the conversation history up to the specified number of tiles.\n    \"\"\"\n\n    # Initialize an empty string to store the conversation history\n    conversation = \"\"\n\n    # Get the length of the conversation history\n    l_conv = len(conversation_history)\n\n    # If the length of the conversation history is greater than k, retrieve the last k tiles\n    if l_conv > k:\n        for i in range(l_conv - k, l_conv):\n            conversation += conversation_history[i]\n    else:\n        # If the length is less than or equal to k, include the entire conversation history\n        for i in conversation_history:\n            conversation += i\n\n    # Return the concatenated conversation history string\n    return conversation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Query Processing","metadata":{}},{"cell_type":"markdown","source":"### Taking the query from the user","metadata":{}},{"cell_type":"markdown","source":"### Convert the query from Indic to English","metadata":{}},{"cell_type":"code","source":"def translate_indic_to_english(user_input,source_language,target_language):\n    # Record the start time for performance measurement\n    begin = time.time()\n    # Check if the source language is different from the target language for transliteration\n    if not (source_language == target_language):\n        # Preprocess the user input batch for transliteration\n        batch = ip_indic_to_en.preprocess_batch([user_input], src_lang=source_language, tgt_lang=target_language)\n        batch = tokenizer_indic_to_en(batch, src=True, return_tensors=\"pt\")\n\n        # Generate transliterations using the pre-trained model\n        with torch.inference_mode():\n            user_input_translated = model_indic_to_en.generate(\n                batch.input_ids.to(\"cuda\"), num_beams=5, num_return_sequences=1, max_length=256\n            )\n\n        # Decode the generated sequences and post-process the transliterations\n        user_input_translated = tokenizer_indic_to_en.batch_decode(user_input_translated, src=False)\n        user_input_translated = ip_indic_to_en.postprocess_batch(user_input_translated, lang=target_language)\n\n        # Print the transliterated user input\n        # print(user_input_translated)\n    else:\n        # If the source and target languages are the same, no transliteration is needed\n        user_input_translated = [user_input]\n    end = time.time()\n    return user_input_translated,end-begin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the vector database for getting some relevant question from the user","metadata":{}},{"cell_type":"code","source":"def get_context(user_input_translated,k=2, fetch_k=50):\n    '''\n    Perform max marginal relevance search using the FAISS vector store\n    Search for relevant documents based on the transliterated user input\n    Usually you would want the fetch_k parameter >> k parameter\n    This is because the fetch_k parameter is the number of documents that will be fetched before filtering. \n    If you set fetch_k to a low number, you might not get enough documents to filter from.\n    k = Number of searches to be returned\n    fetch_k = Number of documents that will be fetched before filtering\n    '''\n    docs = db.max_marginal_relevance_search(user_input_translated[0], k=k, fetch_k=fetch_k) \n    # Initialize an empty string to store concatenated contexts\n    contexts = \"\"\n\n    # Iterate through the retrieved documents and print page content\n    for i, doc in enumerate(docs):\n        # print(f\"{i + 1}. {doc.page_content}\")\n\n        # Concatenate the page content for further processing if needed\n        contexts += doc.page_content + \"\\n\\n\"\n    \n    return contexts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting English output from the Model back to Hindi","metadata":{}},{"cell_type":"code","source":"def translate_english_to_indic(en_text,source_language,target_language):\n    begin = time.time()\n    # Translate English text to Hindi if the source and target languages are different\n    if not (source_language == target_language):\n        # Preprocess the English text for translation\n        batch = ip_en_to_indic.preprocess_batch([en_text], src_lang=target_language, tgt_lang=source_language)\n        batch = tokenizer_en_to_indic(batch, src=True, return_tensors=\"pt\")\n\n        # Generate translations using the pre-trained model\n        with torch.inference_mode():\n            outputs = model_en_to_indic.generate(\n                batch.input_ids.to(\"cuda\"), num_beams=2, num_return_sequences=1, max_length=256\n            )\n\n        # Decode the generated sequences and post-process the translations\n        outputs = tokenizer_en_to_indic.batch_decode(outputs, src=False)\n        outputs = ip_en_to_indic.postprocess_batch(outputs, lang=source_language)\n    else:\n        # If the source and target languages are the same, no translation is needed\n        outputs = [en_text]\n\n    # Record the end time for performance measurement\n    end = time.time()\n\n    # Measure and print the elapsed time for the operation\n    elapsed_time = end - begin\n    \n    return outputs,elapsed_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conversation_history = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conversation(file_path, language_choice, speed_choice, is_audio=False, time_it=True, print_context=False):\n    \"\"\"\n    Conducts a conversation with the chatbot, translating user input, retrieving context, and generating responses.\n\n    Parameters:\n    - user_input (str): The input provided by the user in an Indic language.\n    - time_it (bool): A flag indicating whether to print the time taken for each step (default is True).\n\n    Returns:\n    None\n    \"\"\"\n    # Define a dictionary mapping languages to their corresponding Indic2Trans language codes\n    languages = {\n        \"Assamese\":\"asm_Beng\",\n        \"Kashmiri (Arabic)\":\"kas_Arab\",\n        \"Punjabi\":\"pan_Guru\",\n        \"Bengali\":\"ben_Beng\",\n        \"Kashmiri (Devanagari)\":\"kas_Deva\",\n        \"Sanskrit\":\"san_Deva\",\n        \"Bodo\":\"brx_Deva\",\n        \"Maithili\":\"mai_Deva\",\n        \"Santali\":\"sat_Olck\",\n        \"Dogri\":\"doi_Deva\",\n        \"Malayalam\":\"mal_Mlym\",\n        \"Sindhi (Arabic)\":\"snd_Arab\",\n        \"Marathi\":\"mar_Deva\",\n        \"English\":\"eng_Latn\",\n        \"Sindhi (Devanagari)\":\"snd_Deva\",\n        \"Konkani\":\"gom_Deva\",\n        \"Manipuri (Bengali)\":\"mni_Beng\",\n        \"Tamil\":\"tam_Taml\",\n        \"Gujarati\":\"guj_Gujr\",\n        \"Manipuri (Meitei)\":\"mni_Mtei\",\n        \"Telugu\":\"tel_Telu\",\n        \"Hindi\":\"hin_Deva\",\n        \"Nepali\":\"npi_Deva\",\n        \"Urdu\":\"urd_Arab\",\n        \"Kannada\":\"kan_Knda\",\n        \"Odia\":\"ory_Orya\"\n    }\n    # Select the source language for transliteration based on the user's choice\n    source_language = languages[language_choice]\n\n    # Set the target language for transliteration to English\n    target_language = \"eng_Latn\"\n    # ASR model to convert the audio to text\n    if is_audio:\n        user_input = transcriber(file_path)\n        user_input = user_input[\"text\"]\n    else:\n        user_input = file_path\n\n    # Translate user input from Indic to English\n    user_input_translated, t1_time = translate_indic_to_english(user_input,source_language,target_language)\n\n    # Record the start time for performance measurement\n    begin = time.time()\n\n    if speed_choice == 0:\n        # Get context based on the translated user input\n        context = get_context(user_input_translated,k=1)\n        if print_context:\n            print(context)\n    else:\n        # Get context based on the translated user input\n        context = get_context(user_input_translated)\n        if print_context:\n            print(context)\n\n    # Calculate the time taken for the vector database operation\n    vectordb_time = time.time() - begin\n\n    if speed_choice==1:\n        # Create a system input for the chatbot including context and conversation history\n        system_input = f\"\"\"As a helpful chatbot specialized in providing concise and relevant information about websites, your goal is to offer insightful responses to user queries. Utilize both the provided context and your chat history, ensuring the inclusion of the website mentioned in the context in your responses. Strive to address the user's inquiry in one to two sentences while maximizing assistance.\nChat History: {get_history()}\nContext: {context}\nUser Query: {user_input_translated}\n\"\"\"\n\n        # Record the start time for LLM generation\n        begin = time.time()\n\n        # Generate an English response using the language model pipeline\n        response = llm_model.generate_content(system_input)\n        en_text = response.text\n        print(en_text)\n\n        # Calculate the time taken for the LLM generation\n        llm_time = time.time() - begin\n        \n    elif speed_choice==2:\n        system_input = f\"\"\"As a helpful chatbot specialized in providing concise and relevant information about websites, your goal is to offer insightful responses to user queries. Utilize the provided context, ensuring the inclusion of the website mentioned in the context in your responses. Strive to address the user's inquiry in one to two sentences while maximizing assistance.\nPlease remember to include meaningful links and try to help the users as much as you can.\nContext: {context}\n\"\"\"\n        template = f\"### System:\\n{system_input}\\n### User:\\n{user_input_translated}\\n### Assistant:\\n\"\n\n        # Record the start time for LLM generation\n        begin = time.time()\n\n        # Generate an English response using the language model pipeline\n        en_text = pipe(template)\n\n        # Calculate the time taken for the LLM generation\n        llm_time = time.time() - begin\n\n        # Extract the generated English text from the pipeline response\n        en_text = en_text[0][\"generated_text\"].replace(template, \"\")\n        \n    else:\n        en_text = context\n\n    # Update conversation history with user input and assistant response\n    history = f\"User: {user_input_translated[0]}\\nAssistant: {en_text}\\n\"\n    conversation_history.append(history)\n\n    # Translate the generated English text back to Indic\n    output, t2_time = translate_english_to_indic(en_text,source_language,target_language)\n    \n    if language_choice == \"Hindi\":\n        text = output[0]\n        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            audio_tensor = model(**inputs).waveform\n        torchaudio.save('test_1.mp3',\n                  audio_tensor.cpu(),\n                  sample_rate=16000)\n\n    # Print the translated output\n    return(output[0],user_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Server\n","metadata":{}},{"cell_type":"code","source":"!pip install -q python-multipart multipart fastapi[all] nest-asyncio pyngrok uvicorn python-firebase firebase-admin python-jose passlib pydub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Optional\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI, HTTPException, Form, File, UploadFile\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\nfrom fastapi import FastAPI, Form\nimport base64\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport os\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI, HTTPException, Form\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\nfrom fastapi import FastAPI, Form\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi import HTTPException\nimport base64\nimport re\nimport base64\nimport io\nimport wave\nimport base64\nimport io\nfrom pydub import AudioSegment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_base64_to_mp3(base64_data, output_path=\"/kaggle/working/decoded_audio.mp3\"):\n    try:\n        # Decode base64 audio data\n        audio_bytes = base64.b64decode(base64_data)\n\n        # Create a file-like object from the decoded bytes\n        audio_file = io.BytesIO(audio_bytes)\n\n        # Load audio data using pydub\n        audio_segment = AudioSegment.from_file(audio_file, format=\"wav\")\n\n        # Export the audio to an MP3 file\n        audio_segment.export(output_path, format=\"mp3\")\n\n        print(f\"Audio successfully decoded and saved to {output_path}\")\n        return output_path\n\n    except Exception as e:\n        print(f\"Error decoding audio: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app = FastAPI()\n\n# CORS middleware to allow requests from the frontend\norigins = [\"*\"]  # Adjust as needed\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# JWT Configuration\nSECRET_KEY = \"$ecret@2024!@#\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Dependency to get the current user from the token\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\nclass User(BaseModel):\n    email: str\n    password: str\n        \nclass Query(BaseModel):\n    query: str\n    lang: str\n    model: str    \n\nclass BaseAudio(BaseModel):\n    base64info: str\n    lang: str\n    model: str\n    \n# Firebase user management functions\ndef create_user(email: str, password: str):\n    user = auth.create_user_with_email_and_password(email, password)\n    return user\n\ndef login_user(email: str, password: str):\n    user = auth.sign_in_with_email_and_password(email, password)\n    return user\n\n@app.post(\"/api/register\")\nasync def register(email: str = Form(...), password: str = Form(...)):\n    try:\n        # Check for existing user\n        user_doc = db.collection(\"users\").document(email).get()\n        if user_doc.exists:\n            return {\"success\": False, \"message\": \"User already exists!\"}\n\n        # Create new user document\n        user_data = {\"email\": email, \"password\": password}\n        db.collection(\"users\").document(email).set(user_data)\n\n        return {\"success\": True, \"message\": \"Registration successful\"}\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=\"Registration failed!\")\n\n@app.post(\"/api/login\")\nasync def login(email: str = Form(...), password: str = Form(...)):\n    try:\n        # Retrieve user from Firestore\n        user_doc = db.collection(\"users\").document(email).get()\n        user_data = user_doc.to_dict()\n\n        # Check password\n        if user_data.get(\"password\") != password:\n            raise HTTPException(status_code=401, detail=\"Invalid credentials!\")\n\n        # Implement authentication logic (e.g., JWT)\n        return {\"success\": True, \"message\": \"Login Successful\"}\n\n    except Exception as e:\n        return {\"success\":False, \"message\":\"User does not exist!\"}\n\n@app.post(\"/query\")\nasync def querySF(req: Query):\n    print(req.query)\n    response, _ = conversation(req.query, speed_choice=int(req.model), language_choice=req.lang)\n    ((response.replace('[bhuvan-wbis.nrsc.gov.in]','')).replace(')', ' )')).replace(']', ' ]')\n    return(response)\n    \n\n@app.post(\"/upload-audio\")\nasync def upload_audio(req:BaseAudio):\n    try:\n        decode_base64_to_mp3(req.base64info)\n        # Decode base64 audio data\n        audio_bytes = base64.b64decode(req.base64info)\n\n        # Create a file-like object from the decoded bytes\n        audio_file = io.BytesIO(audio_bytes)\n        output_path=\"/kaggle/working/audio.wav\"\n        with wave.open(output_path, 'wb') as wf:\n            wf.setnchannels(1)  # Mono audio\n            wf.setsampwidth(2)  # 16-bit audio\n            wf.setframerate(44100)  # Sample rate (adjust as needed)\n            wf.writeframes(audio_file.read())\n        print(f\"Audio successfully decoded and saved to {output_path}\")\n        response, user_input = conversation(\"What is Bhuvan?\", speed_choice=int(req.model), language_choice=req.lang, is_audio=False)\n        print(response)\n        return(response)\n\n    except Exception as e:\n        print(f\"Error decoding audio: {e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ngrok config add-authtoken 2ax1IpRRcG4GZhHVmYVQbG0pcFV_6Yeg42KGpZ8J9RtVTzJwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom typing import Optional, Dict\nimport uvicorn\nimport nest_asyncio\nfrom pyngrok import ngrok\n# specify a port\nport = 8000\nngrok_tunnel = ngrok.connect(port, bind_tls=True, hostname=\"sif.ngrok.io\")\n\n# where we can visit our fastAPI app\nprint('Public URL:', ngrok_tunnel.public_url)\n\n\nnest_asyncio.apply()\n\n# finally run the app\nuvicorn.run(app, port=port)\n\n# it will take some moment (~10 sec) to load the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running Samples","metadata":{}},{"cell_type":"code","source":"%%time\nuser_input3 = \"कोरोना के बारे में बताएं\"\nconversation(user_input3,speed_choice=2,language_choice=\"Hindi\",print_context=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input2 = \"मुझे आधार केंद्र बारे में विस्तार में बताएं\"\nconversation(file_path=user_input2,speed_choice=2,language_choice=\"Hindi\",print_context=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input2 = \"What was my previous question?\"\nconversation(file_path=user_input2,language_choice=\"English\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input5 = \"मुझे अब तक हुई हमारी सारी बातचीत का सारांश दीजिए\"\nconversation(user_input5,language_choice=\"Hindi\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input6 = \"भारत के जंगलों के बारे में बताओ\"\nconversation(user_input6,language_choice=\"Hindi\",speed_choice=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input7 = \"मुझे भारत में वनों के बारे में बताएं?\"\nconversation(user_input7,language_choice=\"Hindi\",speed_choice=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input8 = \"मुझे भारत में खदानों के बारे में बताएं?\"\nconversation_history = []\nconversation(user_input8, language_choice=\"Hindi\",speed_choice=1,print_context=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input9 = \"Can you tell me about water resource management and assessment\"\nconversation(user_input9, language_choice=\"English\",speed_choice=2,print_context=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nuser_input9 = \"Can you give me the website for the technical documentation of WBIS\"\nconversation(user_input9, language_choice=\"English\",speed_choice=0,print_context=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat.history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}